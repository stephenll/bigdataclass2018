```{r, modeling, include = FALSE}
knitr::opts_chunk$set(eval = as.logical(Sys.getenv("evaluate")))
```

# Modeling

```{r, catchup5, include = FALSE}
library(tidyverse)
library(DBI)
library(dbplyr)
library(dbplot)
library(tidypredict)
# Class catchup
con <- DBI::dbConnect(odbc::odbc(), "Postgres Dev")
airports <- tbl(con, in_schema("datawarehouse", "airport")) 
table_flights <- tbl(con, in_schema("datawarehouse", "flight"))
carriers <- tbl(con, in_schema("datawarehouse", "carrier"))
set.seed(100)
```

## SQL Native sampling 
*Use PostgreSQL TABLESAMPLE clause*

1. Find out the class of the object returned by `show_query()`.  Test with *table_flights*.
```{r}
table_flights %>%
  show_query() %>%
  class()
```

2. Find out the class of the object returned by `remote_query()`. Test with *table_flights*.
```{r}
table_flights %>%
  remote_query() %>%
  class()
```

3. Run `remote_query()` again *table_flights*
```{r}
remote_query(table_flights)
```

4. Use `build_sql()` to paste together the results of the `remote_query()` operation and *" TABLESAMPLE SYSTEM (0.1)"*
```{r}
build_sql(remote_query(table_flights), " TABLESAMPLE SYSTEM (0.1)")
```

5. Use `build_sql()` and `remote_query()` to combine a the `dplyr` command with a custom SQL statement
```{r}
sql_sample <-  dbGetQuery(con, build_sql(remote_query(table_flights), " TABLESAMPLE SYSTEM (0.1)"))
```

6. Preview the sample data
```{r}
sql_sample
```

6. Test the efficacy of the sampling using `dbplot_histogram()` and comparing to the histogram produced in the Visualization chapter.
```{r}
dbplot_histogram(sql_sample, distance)
```

## Sample with ID
*Use a record's unique ID to produce a sample*

1. Summarize with `max()` and `min()` to get the upper and lower bound of *flightid*
```{r}
limit <- table_flights %>%
  summarise(
    max = max(flightid, na.rm = TRUE),
    min = min(flightid, na.rm = TRUE)
  ) %>%
  collect()
```

2. Use `sample()` to get 0.1% of IDs
```{r}
sampling <- sample(
  limit$min:limit$max, 
  round((limit$max -limit$min) * 0.001)
  )
```

3. Use `%in%` to match the sample IDs in *table_flights* table
```{r}
id_sample <- table_flights %>%
  filter(flightid %in% sampling) %>%
  collect()
```

4. Test the efficacy of the sampling using `dbplot_histogram()` and comparing to the histogram produced in the Visualization chapter.
```{r}
dbplot_histogram(id_sample, distance)
```


## Sample manually
*Use `row_number()`, `sample()` and `map_df()` to create a sample data set*

1. Create a filtered data set with January's data
```{r}
db_month <- table_flights %>%
  filter(month == 1)
```

2. Get the row count, collect and save the results to a variable
```{r}
rows <- db_month %>%
  tally() %>%
  pull() %>%
  as.integer()

rows
```

3. Use `row_number()` to create a new column to number each row
```{r}
db_month <- db_month %>%
  mutate(row = row_number()) 
```

4. Create a random set of 600 numbers, limited by the number of rows
```{r}
sampling <- sample(1:rows, 600)
```

5. Use `%in%` to filter the matched sample row IDs with the random set
```{r}
db_month <- db_month %>%
  filter(row %in% sampling)
```

6. Verify number of rows
```{r}
tally(db_month)
```

7. Create a function with the previous steps, but replacing the month number with an argument.  Collect the data at the end
```{r}
sample_segment <- function(x, size = 600) {
  
  db_month <- table_flights %>%
    filter(month == x)
  
  rows <- db_month %>%
    tally() %>%
    pull() %>%
    as.integer()

  
  db_month <- db_month %>%
    mutate(row = row_number())
  
  sampling <- sample(1:rows, size)
  
  db_month %>%
    filter(row %in% sampling) %>%
    collect()
}
```

8. Test the function
```{r}
head(sample_segment(3), 100)
```

9. Use `map_df()` to run the function for each month
```{r}
strat_sample <- 1:12 %>%
  map_df(~sample_segment(.x))
```

10. Verify sample with a `dbplot_histogram()`
```{r}
dbplot_histogram(strat_sample, distance)
```

## Create a model & test

1. Prepare a model data set.  Using `case_when()` create a field called *season* and assign based 
```{r}
model_data <- strat_sample %>%
    mutate(
    season = case_when(
      month >= 3 & month <= 5  ~ "Spring",
      month >= 6 & month <= 8  ~ "Summer",
      month >= 9 & month <= 11 ~ "Fall",
      month == 12 | month <= 2  ~ "Winter"
    )
  ) %>%
  select(arrdelay, season, depdelay) 
  
```

2. Create a simple `lm()` model against *arrdelay*
```{r}
model_lm <- lm(arrdelay ~ . , data = model_data)
summary(model_lm)
```

3. Create a test data set by combining the sampling and model data set routines
```{r}
test_sample <- 1:12 %>%
  map_df(~sample_segment(.x, 100)) %>%
    mutate(
    season = case_when(
      month >= 3 & month <= 5  ~ "Spring",
      month >= 6 & month <= 8  ~ "Summer",
      month >= 9 & month <= 11 ~ "Fall",
      month == 12 | month <= 2  ~ "Winter"
    )
  ) %>%
  select(arrdelay, season, depdelay) 
  
```

4. Run a simple routine to check accuracy 
```{r}
test_sample %>%
  mutate(p = predict(model_lm, test_sample),
         over = abs(p - arrdelay) < 10) %>%
  group_by(over) %>% 
  tally() %>%
  mutate(percent = round(n / sum(n), 2))
```

## Score inside database
*Learn about tidypredict to run predictions inside the database*

1. Load the library, and see the results of passing the model as an argument to `tidypredict_fit()` 
```{r}
library(tidypredict)

tidypredict_fit(model_lm)
```

2. Use `tidypredict_sql()` to see the resulting SQL statement
```{r}
tidypredict_sql(model_lm, con)
```

3. Run the prediction inside `dplyr` by piping the same transformations into `tidypredict_to_column()`, but starting with *table_flights*

```{r}
table_flights %>%
  filter(month == 2,
         dayofmonth == 1) %>%
    mutate(
    season = case_when(
      month >= 3 & month <= 5  ~ "Spring",
      month >= 6 & month <= 8  ~ "Summer",
      month >= 9 & month <= 11 ~ "Fall",
      month == 12 | month <= 2  ~ "Winter"
    )
  ) %>%
  select( season, depdelay) %>%
  tidypredict_to_column(model_lm) %>%
  head()
```

4. View the SQL behind the `dplyr` command with `remote_query()`
```{r}
table_flights %>%
  filter(month == 2,
         dayofmonth == 1) %>%
    mutate(
    season = case_when(
      month >= 3 & month <= 5  ~ "Spring",
      month >= 6 & month <= 8  ~ "Summer",
      month >= 9 & month <= 11 ~ "Fall",
      month == 12 | month <= 2  ~ "Winter"
    )
  ) %>%
  select( season, depdelay) %>%
  tidypredict_to_column(model_lm) %>%
  remote_query()
```

5. Compare predictions to ensure results are within range using `tidypredict_test()`
```{r}
test <- tidypredict_test(model_lm)
test
```

6. View the records that exceeded the threshold
```{r}
test$raw_results %>%
  filter(fit_threshold)
```

## Parsed model
*Quick review of the model parser*

1. Use the `parse_model()` function to see how `tidypredict` interprets the model
```{r}
pm <- parse_model(model_lm)
```

2. With `tidypredict_fit()`, verify that the resulting table can be used to get the fit formula
```{r}
tidypredict_fit(pm)
```

3. Save the parsed model for later use using the `yaml` package
```{r}
library(yaml)

write_yaml(pm, "my_model.yml")
``` 

4. Reload model from the YAML file

```{r}
my_pm <- read_yaml("my_model.yml")
```

5. Use the reloaded model to build the fit formula

```{r}
tidypredict_fit(my_pm)
```

## Model inside the database
*Brief intro to modeldb*

1. Load `modeldb`
```{r}
library(modeldb)
```

2. Use the `sampling` variable to create a filtered table of `table_flights`
```{r}
sample <- table_flights %>%
  filter(flightid %in% sampling) 
```

3. Select *deptime*, *distance* and *arrdelay* from `sample` and pipe into `linear_regression_db()`, pass *arrdelay* as the only argument.
```{r}
sample %>%
  select(deptime, distance, arrdelay) %>%
  linear_regression_db(arrdelay)
```

4. Using the coefficients from the results, create a new column that multiplies each field against the corresponding coefficient, and adds them all together along with the intercept
```{r}
table_flights %>%
  head(1000) %>%
  mutate(pred = --0.6262197	 + (deptime * 0.01050023	) + (distance * -0.003834017	)) %>%
  select(arrdelay, pred) 
```

5. Add *dayofweek* to the variable selection.  Add `add_dummy_variables()` in between the selection and the linear regression function. Pass *dayofweek* and `c(1:7)` to represent the 7 days
```{r}
sample %>%
  select(deptime, distance, arrdelay, dayofweek) %>%
  add_dummy_variables(dayofweek, c(1:7)) %>%
  linear_regression_db(arrdelay, sample_size = 32) 
```

6. Replace *arrdelay* with *uniquecarrier*. Group by *uniquecarrier* and remove the `add_dummy_variable()`.
```{r}
sample %>%
  select(deptime, distance, arrdelay, uniquecarrier) %>%
  group_by(uniquecarrier) %>%
  linear_regression_db(arrdelay)
```

7. Pipe the code into `ggplot2`.  Use the intercept for the plot's `x` and *uniquecarrier* for `y`.  Use `geom_point()`
```{r}
sample %>%
  select(deptime, distance, arrdelay, uniquecarrier) %>%
  group_by(uniquecarrier) %>%
  linear_regression_db(arrdelay) %>%
  ggplot() + 
  geom_point(aes(`(Intercept)`, uniquecarrier))
```

```{r, include = FALSE}
dbDisconnect(con)
```





  



